{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_dataset(ham_url:str=HAM_URL, spam_url:str=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for file_name, url in ((\"ham.ter.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, file_name)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_spam_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spam', 500)\n",
      "('Ham', 2500)\n"
     ]
    }
   ],
   "source": [
    "for item in zip([\"Spam\", \"Ham\"],[len(spam_filenames), len(ham_filenames)]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email \n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam:bool, file_name:str, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, file_name), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ham_emails = [load_email(is_spam=False, file_name=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, file_name=name) for name in spam_filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[0].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought you might like these:\n",
      "1) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822slim1\n",
      "\n",
      "2) Fight The Risk of Cancer! \n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822nic1 \n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice \n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822ppl1\n",
      "\n",
      "Offer Manager\n",
      "Daily-Deals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?social@linux.ie%7C29%7C134077\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group Social Events: social@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/social for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[4].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return f\"multipart({','.join([get_email_structure(sub_email) for sub_email in payload])})\"\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s_ham = structures_counter(ham_emails).most_common()\n",
    "s = {}\n",
    "\n",
    "for key,val in s_ham:\n",
    "    s[key] = val\n",
    "df = pd.Series(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text/plain                                                                                                               0.9632\n",
       "multipart(text/plain,application/pgp-signature)                                                                          0.0264\n",
       "multipart(text/plain,text/html)                                                                                          0.0032\n",
       "multipart(text/plain,text/plain)                                                                                         0.0016\n",
       "multipart(text/plain)                                                                                                    0.0012\n",
       "multipart(text/plain,application/octet-stream)                                                                           0.0008\n",
       "multipart(text/plain,text/enriched)                                                                                      0.0004\n",
       "multipart(text/plain,application/ms-tnef,text/plain)                                                                     0.0004\n",
       "multipart(multipart(text/plain,text/plain,text/plain),application/pgp-signature)                                         0.0004\n",
       "multipart(text/plain,video/mng)                                                                                          0.0004\n",
       "multipart(text/plain,multipart(text/plain))                                                                              0.0004\n",
       "multipart(text/plain,application/x-pkcs7-signature)                                                                      0.0004\n",
       "multipart(text/plain,multipart(text/plain,text/plain),text/rfc822-headers)                                               0.0004\n",
       "multipart(text/plain,multipart(text/plain,text/plain),multipart(multipart(text/plain,application/x-pkcs7-signature)))    0.0004\n",
       "multipart(text/plain,application/x-java-applet)                                                                          0.0004\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df/len(ham_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_spam = structures_counter(spam_emails).most_common()\n",
    "s = {}\n",
    "\n",
    "for key,val in s_spam:\n",
    "    s[key] = val\n",
    "df = pd.Series(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text/plain                                                             0.436\n",
       "text/html                                                              0.366\n",
       "multipart(text/plain,text/html)                                        0.090\n",
       "multipart(text/html)                                                   0.040\n",
       "multipart(text/plain)                                                  0.038\n",
       "multipart(multipart(text/html))                                        0.010\n",
       "multipart(text/plain,image/jpeg)                                       0.006\n",
       "multipart(text/html,application/octet-stream)                          0.004\n",
       "multipart(text/plain,application/octet-stream)                         0.002\n",
       "multipart(text/html,text/plain)                                        0.002\n",
       "multipart(multipart(text/html),application/octet-stream,image/jpeg)    0.002\n",
       "multipart(multipart(text/plain,text/html),image/gif)                   0.002\n",
       "multipart/alternative                                                  0.002\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df / len(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Re: New Sequences Window'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_plain_text(email):\n",
    "    soup = BeautifulSoup(email, 'html.parser')\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        text = soup.new_tag(\"p\")\n",
    "        text.string = \"Hyperlink\"\n",
    "        link.replace_with(text)\n",
    "    return soup.text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<body>\n",
      "\n",
      "<p><font face=\"Arial\" color=\"#FF0000\"><b>Attention U.S. HomeOwners</b></font> </p>\n",
      "\n",
      "<p><font face=\"Arial\"><b>If you want to save an extra $30,000 (average savings)</b></font> </p>\n",
      "<p><font face=\"Arial\"><b>on your mortgage even if you have already refinanced</b></font></p>\n",
      "<p><a href=\"http://www.Smartest_Move_U_Could_Make.com%40w%77%77%2E%74%65%72%72%61%2Ee%73/pe%72%73o%6E%61%6C9/chunk102/\"><font face=\"Arial\"><b>CLICK HERE</b></font></a></p>\n",
      "<p><font face=\"Arial\"><b>We also have the lowest rates and most professional</b></font> </p>\n",
      "<p><font face=\"Arial\"><b>and friendly service you will experience.&nbsp; We will</b></font> </p>\n",
      "<p><font face=\"Arial\"><b>answer your questions with no obligation.</b></font></p>\n",
      "<p><a href=\"http://www.Mortgage_Opportunity_777.com%40w%77%77%2E%74%65%72%72%61%2Ee%73/pe%72%73o%6E%61%6C9/pants105/\"><font face=\"Arial\"><b>CLICK HERE</b></font></a></p>\n",
      "<p><font face=\"Arial\"><b>We have rates as low as 4.65% and Loans for all</b></font></p>\n",
      "<p><font  ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in x_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Attention U.S. HomeOwners \n",
      "If you want to save an extra $30,000 (average savings) \n",
      "on your mortgage even if you have already refinanced\n",
      "Hyperlink\n",
      "We also have the lowest rates and most professional \n",
      "and friendly service you will experience.  We will \n",
      "answer your questions with no obligation.\n",
      "Hyperlink\n",
      "We have rates as low as 4.65% and Loans for all\n",
      "types of people and situations.\n",
      "For those of you who have a mortgage and have been \n",
      "turned down we can still save you around $30,000.\n",
      "Hyperlink for a FREE, friendly\n",
      "quote.\n",
      " \n",
      " \n",
      "If you no longer wish to receive our offers and updates Hyperlink \n",
      " and we will promptly honor your request.\n",
      "\n",
      "\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Attention U.S. HomeOwners \n",
      "If you want to save an extra $30,000 (average savings) \n",
      "on your mortgag ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urlextract\n",
    "import re\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "url_extractor = urlextract.URLExtract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, \n",
    "                 strip_headers=True,\n",
    "                 lower_case=True,\n",
    "                 remove_punctuations=True,\n",
    "                 replace_urls=True,\n",
    "                 replace_numbers=True,\n",
    "                 stemming=True) -> None:\n",
    "        super().__init__()\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuations = remove_punctuations\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        x_transformed = []\n",
    "        for email in x:\n",
    "            text = email_to_text(email) or ''\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text.replace(url, \"URL\")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuations:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            x_transformed.append(word_counts)\n",
    "        return np.array(x_transformed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'s': 2, 'like': 2, 'were': 2, 'ye': 1, 'it': 1, 'nice': 1, 'to': 1, 'be': 1, 'back': 1, 'in': 1, 'america': 1, 'flaccid': 1, 'state': 1, 'seem': 1, 'onli': 1, 'yesterday': 1, 'we': 1, 'suffer': 1, 'electil': 1, 'dysfunct': 1, 'mayb': 1, 'if': 1, 'they': 1, 'made': 1, 'the': 1, 'ballot': 1, 'oval': 1, 'look': 1, 'littl': 1, 'blue': 1, 'pill': 1, 'no': 1, 'serious': 1, 'i': 1, 'm': 1, 'here': 1, 'all': 1, 'week': 1, 'you': 1, 'great': 1, 'nite': 1, 'everybodi': 1}),\n",
       "       Counter({'number': 9, 'the': 7, 'of': 4, 'it': 4, 'and': 3, 's': 3, 'thi': 2, 'gay': 2, 'or': 2, 'numberk': 2, 'in': 2, 'suggest': 2, 'that': 2, 'a': 2, 'rah': 2, 'who': 2, 'for': 2, 'ha': 2, 'been': 2, 'ibuc': 2, 'com': 2, 'at': 1, 'am': 1, 'on': 1, 'gordon': 1, 'mohr': 1, 'wrote': 1, 'calcul': 1, 'elid': 1, 'cours': 1, 'say': 1, 'veri': 1, 'littl': 1, 'almost': 1, 'noth': 1, 'about': 1, 'overal': 1, 'popul': 1, 'behavior': 1, 'straight': 1, 'rel': 1, 'preval': 1, 'individu': 1, 'either': 1, 'group': 1, 'but': 1, 'doe': 1, 'strongli': 1, 'male': 1, 'with': 1, 'partner': 1, 'exist': 1, 'measur': 1, 'so': 1, 'peopl': 1, 'should': 1, 'stop': 1, 'treat': 1, 'eugen': 1, 'anecdot': 1, 'estim': 1, 'as': 1, 'if': 1, 'were': 1, 'sheer': 1, 'fantasi': 1, 'bitbitch': 1, 'own': 1, 'citat': 1, 'otherwis': 1, 'math': 1, 'stuff': 1, 'um': 1, 'bitch': 1, 'i': 1, 'nit': 1, 'cheer': 1, 'could': 1, 'care': 1, 'less': 1, 'boink': 1, 'whom': 1, 'how': 1, 'much': 1, 'though': 1, 'watch': 1, 'righteou': 1, 'anger': 1, 'tm': 1, 'around': 1, 'here': 1, 'morn': 1, 'amus': 1, 'r': 1, 'hettinga': 1, 'mailto': 1, 'internet': 1, 'bearer': 1, 'underwrit': 1, 'corpor': 1, 'http': 1, 'www': 1, 'farquhar': 1, 'street': 1, 'boston': 1, 'ma': 1, 'usa': 1, 'howev': 1, 'may': 1, 'deserv': 1, 'respect': 1, 'use': 1, 'antiqu': 1, 'predict': 1, 'end': 1, 'world': 1, 'not': 1, 'found': 1, 'agreeabl': 1, 'to': 1, 'experi': 1, 'edward': 1, 'gibbon': 1, 'declin': 1, 'fall': 1, 'roman': 1, 'empir': 1}),\n",
       "       Counter({'hyperlink': 1})], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = x_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35,   0,   1,   1,   2,   0,   1,   2,   0,   1,   2],\n",
       "       [132,   9,   7,   4,   3,   4,   2,   1,   3,   1,   0],\n",
       "       [  1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'the': 2,\n",
       " 'it': 3,\n",
       " 's': 4,\n",
       " 'of': 5,\n",
       " 'in': 6,\n",
       " 'were': 7,\n",
       " 'and': 8,\n",
       " 'to': 9,\n",
       " 'like': 10}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.974) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.980) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.989) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9809523809523809"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Polynomial Features\n",
    "\n",
    "Once you plot features against label and realize that they are not linearly corelated but rather have a higher order polynomial relationship e.g. 2\n",
    "<br>\n",
    "The folloeing code adds a polynomial feature to the training dataset with a degree of int\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=int, include_bias=False)\n",
    "x_poly_transformed = poly_features.fit_transform(x)\n",
    "```\n",
    "The new array is then fed to a training algorithm to define its weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "\n",
    "The following code runs for maximum a epochs (max_iter=a) or until the loss drops by less than b during one epoch (tol=b), starting with a learning rate of c\n",
    "(eta0=c),\n",
    "```python\n",
    "from sklearn.linear_model import SDG(Regressor or Classifier)\n",
    "sgd_reg = SGDRegressor(max_iter=a, tol=b, penalty=None, eta0=c)\n",
    "sgd_reg.fit(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regressor\n",
    "\n",
    "Ridge linear algorithm is a regularized version of the linear model which puts some additional weight on the co-efficients ( alpha ) to prevent the model from overfitting\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge \n",
    "\n",
    "ridge = Ridge(alpha=alpha, solver=\"cholesky\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regressor \n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression. \n",
    "<br>\n",
    "An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features \n",
    "<br>\n",
    "This model allows for a regularization factor of alpha be place on the co-efficients to put some weight on them \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "lasso = Lasso(alpha=alpha)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio l1_ratio. You can also determine the regularization variable with the alpha parameter\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet = ElasticNet(alpha=int, l1_ratio=float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisiticRegression()\n",
    "log_reg.fit(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft max Regression \n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers . This is called Softmax Regression, or Multinomial Logistic Regression\n",
    "<br>\n",
    "regularization is controlled using the C hypervariable\n",
    "\n",
    "```python \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax = LogisticRegression(multi_class=\"multinominal\", solver=\"lbfgs\", c=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "They are algorithms which work by splitting a dataset based on certain features and hence make no generalization of the dataset It stops recursing once it reaches the maximum depth (defined by the ***max_depth*** hyperparameter), or if it cannot\n",
    "find a split that will reduce impurity.\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "**Criterion**: This decides the function used to measure impurity within each node. \n",
    "<br>These parameters include: \"gini\" or \"entropy\"\n",
    "min_samples_split (the minimum number of samples a node must have before it can be split),<br> min_samples_leaf (the minimum num‐\n",
    "ber of samples a leaf node must have)<br>\n",
    "max_leaf_nodes (maximum number of leaf nodes), <br>and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). \n",
    ">Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "This is a method which involves training multiple algorithms and then making predictions based on their collective ( average ) response\n",
    "<hr>\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForrestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "svm_clf = SVC()\n",
    "rand_frst = RandomForrestClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[(\"lr\", log_clf), (\"rf\", rand_frst), (\"svc\", svm_clf)]\n",
    "                              voting=\"hard\")\n",
    "\n",
    "```\n",
    "<hr>\n",
    "Hard voting is based on a simple rule = Majority carries the vote\n",
    "<br>\n",
    "With soft voting, you take advantage of  predict_proba() method, average the probabilities and then return a class based of the average probabilities \n",
    "<br>\n",
    "This is not the case of the SVC class by default, so you need to set its probability hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add a predict_proba() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and pasting\n",
    "It uses the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging(short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\n",
    "The following code trains an ensemble of 500 Decision Tree classifiers, each trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores):\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassfier\n",
    "bag_clf = BaggingClassifier( DecisionTreeClassifier(), \n",
    "                            n_estimators=500,\n",
    "                            max_samples=100, \n",
    "                            bootstrap=True, \n",
    "                            n_jobs=-1)\n",
    "```\n",
    "### Out of bag score\n",
    "\n",
    "```python\n",
    "bag_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "# to view the oob score\n",
    "bag_clf.oob_score_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "As we have discussed, a Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST\n",
    "This is a different python library separate from sklearn\n",
    "```python\n",
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction by PCA \n",
    "\n",
    "This is a method of reducing the number of reatures within a dataset while managing to maintain its variance. n_components determines the degree of variance you wish to preserve\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "x_reduced = pca.fit_transform(x)\n",
    "\n",
    "# decompression\n",
    "x_returned = pca.inverse_transform(x_reduced)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    " Fortunately, Incremental PCA (IPCA) algorithms have been developed: you can split the training\n",
    "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. \n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced = inc_pca.transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means \n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "y_preds.fit_predict(x)\n",
    "# setting predefined centroids\n",
    "kmeans = KMeans(n_clusters=5, init=list_of_centroids, n_init=1)\n",
    "```\n",
    "An important way of checking if the cluster has the right number of clusters is by the  silhouette score\n",
    "```python\n",
    "sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, kmeans.labels_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=int, min_samples=int)\n",
    "```\n",
    "This works for non linear clusterinf, with eps setting the value for the distance round the core values it must consider, and min_samples being the minimum number of samples within the cluster\n",
    "<br>\n",
    "The dbscan algorithm can not be used to predict which cluster a new instance belongs to hence it is coupled with other algorithms for that to be possible\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gausian Mixture \n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=3, n_init=10)\n",
    "gm.fit(X)\n",
    "```\n",
    "\n",
    "This further can be used to find outliers after clustering the data. \n",
    "```python\n",
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]\n",
    "```\n",
    "The decision boundry is 4% in this model.<br>\n",
    "One method of evaluating the number of clusters ( n_components ), or if the model is good is by measuring the AIC or BIC, with the BIC more preferred <br>\n",
    "`gm.aic(x)`\n",
    "<br>\n",
    "`gm.bic(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianGaussianMixture\n",
    "Rather than manually searching for the optimal number of clusters, it is possible to use instead the BayesianGaussianMixture class which is capable of giving weights equal (or close) to zero to unnecessary clusters. Just set the number of clusters n_coMponents to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically. \n",
    "\n",
    "```python\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n",
    "bgm.fit(X)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
